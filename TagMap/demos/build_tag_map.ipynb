{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693e82c-73d6-4ada-acd4-c376de46b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0322ee2-0365-4df4-9891-33532dcd1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tag_mapping.datasets.matterport import (\n",
    "    read_matterport_image_file,\n",
    "    read_matterport_depth_file,\n",
    "    read_matterport_pose_file,\n",
    "    read_matterport_intrinsics_file,\n",
    "    MatterportFilenameBridge\n",
    ")\n",
    "\n",
    "from tag_mapping.models import RAMTagger\n",
    "\n",
    "from tag_mapping.filtering import (\n",
    "    compute_unlikely_tags_center_crop_ensemble,\n",
    "    valid_depth_frame,\n",
    ")\n",
    "\n",
    "from tag_mapping import TagMap, TagMapEntry\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e1596-5c63-4511-af72-0cb8517ef991",
   "metadata": {},
   "source": [
    "## Load scene data\n",
    "Please first download the demo data by running `download_demo_data.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf1d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_dir = 'demo_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0337a-fcf5-4e94-81ff-b67dd8021697",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(scene_dir, 'color')\n",
    "depths_dir = os.path.join(scene_dir, 'depth')\n",
    "poses_dir = os.path.join(scene_dir, 'poses')\n",
    "mesh_path = os.path.join(scene_dir, 'mesh.ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4852671-44b9-4e27-a970-430439b4413c",
   "metadata": {},
   "source": [
    "Load and visualize the scene mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab597ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_mesh = o3d.io.read_triangle_mesh(mesh_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd03fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([scene_mesh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186acc0",
   "metadata": {},
   "source": [
    "Read the camera intrinsics. For simplicity, let's assume that the intrinsics of the camera are fixed across all views of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc79a0-51fd-4c53-b235-06404cefd9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height, fx, fy, cx, cy, d = read_matterport_intrinsics_file(\n",
    "    os.path.join(scene_dir, 'intrinsics.txt')\n",
    ")\n",
    "\n",
    "intrinsics = {\n",
    "    'width': width, 'height': height,\n",
    "    'fx': fx, 'fy': fy,\n",
    "    'cx': cx, 'cy': cy,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c6d65-f182-41e8-86e4-1b2561bfcb6c",
   "metadata": {},
   "source": [
    "## Build a Tag Map of the scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd372d8-34c0-44b8-8949-4d967fb6ea54",
   "metadata": {},
   "source": [
    "### Image tagging model\n",
    "First load the image tagging model which that will generate tags for the scene's viewpoints. We use the [Recognize Anything](https://github.com/xinyu1205/recognize-anything) set of image tagging models. \n",
    "\n",
    "Set `ram_pretrained_path` to the path of the downloaded the image tagging model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68670029",
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_pretrained_path = 'ram_swin_large_14m.pth'\n",
    "\n",
    "ram_tagger = RAMTagger(\n",
    "    config={\n",
    "        'ram_pretrained_path': ram_pretrained_path,\n",
    "        'ram_image_size': 384,\n",
    "        'vit': 'swin_l',\n",
    "        'device': 'cuda',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1e774-9aa8-46fb-8446-5acf8973985a",
   "metadata": {},
   "source": [
    "### Crop ensemble filtering\n",
    "We use a cropped augmented ensemble to help remove false positive tag detections by the model.\n",
    "\n",
    "First, a set of tags is generated for the unmodified image. Modified versions of the image are created by center cropping the unmodified image with different crop proportions. The modified images are also passed through the model. The final set of tags are the tags consistent across all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba488fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_ensemble_proportions = [0.05, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(image, tagging_model):\n",
    "    out = tagging_model.tag_image(image)\n",
    "    tags, confidences = (out['tags'], out['confidences'])\n",
    "\n",
    "    unlikely_tags = compute_unlikely_tags_center_crop_ensemble(\n",
    "        image, tags,\n",
    "        crop_ensemble_proportions,\n",
    "        tagging_model\n",
    "    )\n",
    "\n",
    "    filtered_tags = [tag for tag in tags if tag not in unlikely_tags]\n",
    "    filtered_tag_confidences = [conf for tag, conf in zip(tags, confidences) if tag not in unlikely_tags]\n",
    "    \n",
    "    return filtered_tags, filtered_tag_confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0aadb7-fb80-430d-9faf-9e71cbc0095f",
   "metadata": {},
   "source": [
    "### Tag Map construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b3e2a-3314-492e-acd0-364a7381b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map_metadata = {\n",
    "    'scene_name': os.path.basename(scene_dir),\n",
    "    'intrinsics': intrinsics,\n",
    "}\n",
    "\n",
    "tag_map = TagMap(metadata=tag_map_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2e161-585d-4508-8479-6f9beb219590",
   "metadata": {},
   "source": [
    "Iterate over the scene viewpoints, generating tags for each pushing them to the Tag Map. \n",
    "\n",
    "Viewpoints are skipped if they are likely to be an uninformative view by checking the depth image statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_mean_threshold = 0.6\n",
    "depth_quantile_thresholds = [(0.5, 0.6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cebe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image_filename in tqdm(os.listdir(images_dir)):\n",
    "    filename_bridge = MatterportFilenameBridge.from_image_filename(image_filename)\n",
    "    depth_filename = filename_bridge.depth_filename\n",
    "    pose_filename = filename_bridge.pose_filename\n",
    "\n",
    "    image = read_matterport_image_file(os.path.join(images_dir, image_filename))\n",
    "    depth, depth_image = read_matterport_depth_file(os.path.join(depths_dir, depth_filename))\n",
    "    T_cam_to_world = read_matterport_pose_file(os.path.join(poses_dir, pose_filename))\n",
    "    \n",
    "    # skip viewpoints with invalid depth\n",
    "    if not valid_depth_frame(\n",
    "        depth,\n",
    "        mean_threshold=depth_mean_threshold,\n",
    "        quantile_thresholds=depth_quantile_thresholds\n",
    "    ):\n",
    "        continue\n",
    "    \n",
    "    # get viewpoint tags\n",
    "    tags, confidences = generate_tags(image, ram_tagger)\n",
    "    \n",
    "    # get depth statistics to store for the viewpoint\n",
    "    depth_percentiles = {\n",
    "        str(q): dq for q, dq in zip([0.8], np.quantile(depth, [0.8]))\n",
    "    }\n",
    "    \n",
    "    # add entry corresponding to the viewpoint\n",
    "    entry_uuid = uuid.uuid4()\n",
    "    entry = TagMapEntry(\n",
    "        pose=T_cam_to_world,\n",
    "        uuid=entry_uuid,\n",
    "        extras={\n",
    "            'depth_percentiles': depth_percentiles,\n",
    "            \n",
    "            # Here we store the viewpoint image filenames for visualization purposes,\n",
    "            # but it is not necessary to store these.\n",
    "            'image_filename': image_filename,\n",
    "            'depth_filename': depth_filename,\n",
    "        }\n",
    "    )\n",
    "    tag_map.add_entry(entry)\n",
    "    \n",
    "    # add information on the tags of the viewpoint\n",
    "    for tag, conf in zip(tags, confidences):\n",
    "        tag_map.add_tag(\n",
    "            tag,\n",
    "            entry_uuid,\n",
    "\n",
    "            # Here we also store the image tagging model confidence score for all viewpoint tags,\n",
    "            # but currently this is information is not used downstream.\n",
    "            extras={\n",
    "                'confidence': conf,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0da47d",
   "metadata": {},
   "source": [
    "## Inspect the Tag Map\n",
    "A tag in recognized in the scene can be selected for visualization. The tags are listed in a dropdown along with how many viewpoints that tag was recognized in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = []\n",
    "for tag in sorted(tag_map.unique_objects):\n",
    "    options.append(\n",
    "        (tag + '  (' + str(len(tag_map.query(tag))) + ')', tag)\n",
    "    )\n",
    "\n",
    "query_dropdown = widgets.Dropdown(options=options, description='Select a tag:')\n",
    "display(query_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13496360-31ab-4572-980c-1b17ef1f7343",
   "metadata": {},
   "source": [
    "Retrieve corresponding viewpoints for the selected tag.\n",
    "\n",
    "__Rerun this block after changing the selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51894098",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_entries = tag_map.query(query_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777fd2f-f370-486f-8e2b-0a1d1431eeef",
   "metadata": {},
   "source": [
    "Show the images for a few of the viewpoints corresponding to the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ebb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_show = 6\n",
    "num_show = min(len(query_entries), max_show)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_show, figsize=(3*num_show, 6))\n",
    "\n",
    "for i in range(num_show):\n",
    "    entry = query_entries[i]\n",
    "    image_filename = entry.extras['image_filename']\n",
    "    conf = entry.extras['confidence']\n",
    "    \n",
    "    image = read_matterport_image_file(\n",
    "        os.path.join(images_dir, image_filename))\n",
    "    \n",
    "    try:\n",
    "        ax = axes[i]\n",
    "    except TypeError:\n",
    "        ax = axes\n",
    "        \n",
    "    ax.imshow(image)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'confidence: {conf:.2f}')\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc752dd-ceb1-48e2-83f0-d603dd35e727",
   "metadata": {},
   "source": [
    "Visualize the viewpoints corresponding to the tag in the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d_intrinsics = o3d.camera.PinholeCameraIntrinsic()\n",
    "o3d_intrinsics.set_intrinsics(**intrinsics)\n",
    "\n",
    "query_frustum_geometries = []\n",
    "for entry in query_entries:\n",
    "    frame_frustum = o3d.geometry.LineSet.create_camera_visualization(\n",
    "        intrinsic=o3d_intrinsics,\n",
    "        extrinsic=np.linalg.inv(entry.pose),\n",
    "    )\n",
    "    frame_frustum.paint_uniform_color(np.random.rand(3))\n",
    "    query_frustum_geometries.append(frame_frustum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([scene_mesh] + query_frustum_geometries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14fdb09-4937-4b39-8807-6364ae22aa18",
   "metadata": {},
   "source": [
    "##\n",
    "Please see `localization.ipynb` to see how the retrieved viewpoints can be used to generate 3D localizations for a tag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
